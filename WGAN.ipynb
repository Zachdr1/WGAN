{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0.0-beta1\n",
      "GPU Available:  True\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import (\n",
    "    Conv2D,\n",
    "    LeakyReLU,\n",
    "    Dropout,\n",
    "    BatchNormalization,\n",
    "    Flatten,\n",
    "    Dense,\n",
    "    Activation,\n",
    "    Input,\n",
    "    Reshape,\n",
    "    Conv2DTranspose\n",
    ")\n",
    "print(tf.__version__)\n",
    "print(\"GPU Available: \", tf.test.is_gpu_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WGAN():\n",
    "    def __init__(self, dataset, lr=5e-5, c=0.01, m=64, n_critic=5, min_z=-1, max_z=1):\n",
    "        # Config\n",
    "        self.lr = lr\n",
    "        self.c = c\n",
    "        self.m = m\n",
    "        self.n_critic = 5\n",
    "        self.min_z = min_z\n",
    "        self.max_z = max_z\n",
    "        \n",
    "        # Models\n",
    "        self.C = self.critic()\n",
    "        self.G = self.generator()\n",
    "        \n",
    "        # Data\n",
    "        self.dataset = dataset\n",
    "        self.data_iter = tf.compat.v1.data.make_one_shot_iterator(dataset)\n",
    "        \n",
    "        # Optimizers\n",
    "        self.C_opt = tf.keras.optimizers.RMSprop(learning_rate=self.lr)\n",
    "        self.G_opt = tf.keras.optimizers.RMSprop(learning_rate=self.lr)\n",
    "        \n",
    "    @tf.function\n",
    "    def C_loss(self, C_real, C_fake):\n",
    "        return tf.reduce_mean(C_real) - tf.reduce_mean(C_fake)\n",
    "    \n",
    "    @tf.function\n",
    "    def G_loss(self, C_fake):\n",
    "        return -tf.reduce_mean(C_fake)\n",
    "    \n",
    "    @tf.function\n",
    "    def C_grad(self, real_inp, fake_inp):\n",
    "        with tf.GradientTape() as tape:\n",
    "            C_real = self.C(real_inp, training=True)\n",
    "            C_fake = self.C(fake_inp, training=True)\n",
    "            loss = self.C_loss(C_real, C_fake)\n",
    "        return tape.gradient(loss, self.C.trainable_variables)\n",
    "    \n",
    "    @tf.function\n",
    "    def G_grad(self, z):\n",
    "        with tf.GradientTape() as C_tape, tf.GradientTape() as G_tape:\n",
    "            fake = self.G(z, training=True)\n",
    "            C_fake = self.C(fake, training=False)\n",
    "            loss = self.G_loss(C_fake)\n",
    "        return C_tape.gradient(loss, self.G.trainable_variables)\n",
    "    \n",
    "    def clip_weights(self):\n",
    "        for l in self.C.layers:\n",
    "            weights = l.get_weights()\n",
    "            weights = [tf.clip_by_value(w, -self.c, self.c) for w in weights]\n",
    "            l.set_weights(weights)\n",
    "    \n",
    "    @tf.function\n",
    "    def C_train_on_batch(self):\n",
    "        z = tf.random.uniform((self.m, 100,), self.min_z, self.max_z)\n",
    "        fake_inp = self.G(z, training=False)\n",
    "        real_inp = self.get_data_batch()\n",
    "        grad = self.C_grad(real_inp, fake_inp)\n",
    "        self.C_opt.apply_gradients(zip(grad, self.C.trainable_variables))\n",
    "    \n",
    "    @tf.function\n",
    "    def G_train_on_batch(self):\n",
    "        z = tf.random.uniform((self.m, 100,), self.min_z, self.max_z)\n",
    "        grad = self.G_grad(z)\n",
    "        self.G_opt.apply_gradients(zip(grad, self.G.trainable_variables))\n",
    "\n",
    "    @tf.function\n",
    "    def get_data_batch(self):\n",
    "        return self.data_iter.get_next()\n",
    "    \n",
    "    def critic(self):\n",
    "        dropout_prob = .4\n",
    "\n",
    "        inputs = Input(shape=(32, 32, 3))\n",
    "\n",
    "        # Input size = 32x32x3\n",
    "        x = Conv2D(filters=128, kernel_size=5, padding='same', strides=(2, 2), input_shape=(32, 32, 3))(inputs)\n",
    "        x = LeakyReLU(0.2)(x)\n",
    "        # Output size = 16x16x128\n",
    "\n",
    "        # Input size = 16x16x128\n",
    "        x = Conv2D(filters=256, kernel_size=5, padding='same', strides=(2, 2))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(0.2)(x)\n",
    "        # Output size = 8x8x256\n",
    "\n",
    "        # Input size = 8x8x256\n",
    "        x = Conv2D(filters=512, kernel_size=5, strides=(2, 2), padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(0.2)(x)\n",
    "        # Output size = 4x4x512\n",
    "\n",
    "        # Input size = 4x4x512\n",
    "        x = Conv2D(filters=1024, kernel_size=5, padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(0.2)(x)\n",
    "        # Output size = 4x4x1024\n",
    "\n",
    "        # Input size = 4x4x1024\n",
    "        x = Flatten()(x)\n",
    "        out = Dense(1)(x)\n",
    "\n",
    "        net = Model(inputs=inputs, outputs=out)\n",
    "\n",
    "        return net\n",
    "    \n",
    "    def generator(self):\n",
    "        dropout_prob = .4\n",
    "\n",
    "        # Input size = 100\n",
    "        inputs = Input(shape=(100,))\n",
    "        x = Dense(4*4*1024, input_shape=(100,))(inputs)\n",
    "        x = Reshape(target_shape=(4, 4, 1024))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(0.2)(x)\n",
    "        x = Dropout(dropout_prob)(x)\n",
    "        # Output size = 4x4x1024\n",
    "\n",
    "        # Input size = 4x4x1024\n",
    "        x = Conv2DTranspose(filters=512, kernel_size=5, strides=(2, 2), padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(0.2)(x)\n",
    "        x = Dropout(dropout_prob)(x)\n",
    "        # Output size = 8x8x512\n",
    "\n",
    "        # Input size = 8x8x512\n",
    "        x = Conv2DTranspose(filters=256, kernel_size=5, strides=(2, 2), padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(0.2)(x)\n",
    "        x = Dropout(dropout_prob)(x)\n",
    "        # Output size = 16x16x256\n",
    "\n",
    "        # Input size = 16x16x256\n",
    "        x = Conv2DTranspose(filters=128, kernel_size=5, strides=(2, 2), padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(0.2)(x)\n",
    "        x = Dropout(dropout_prob)(x)\n",
    "        # Output size = 32x32x128\n",
    "\n",
    "        # Input size = 32x32x128\n",
    "        x = Conv2DTranspose(filters=3, kernel_size=5, padding='same')(x)\n",
    "        out = Activation('tanh')(x)\n",
    "        # Output size = 32x32x3\n",
    "\n",
    "        net = Model(inputs=inputs, outputs=out)\n",
    "\n",
    "        return net\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "def critic():\n",
    "        dropout_prob = .4\n",
    "\n",
    "        inputs = Input(shape=(128, 128, 3))\n",
    "\n",
    "        # Input size = 128x128x3\n",
    "        x = Conv2D(filters=128, kernel_size=5, padding='same', input_shape=(128, 128, 3))(inputs)\n",
    "        x = LeakyReLU(0.2)(x) # size = 64x64\n",
    "        # Output size = 64x64x128\n",
    "\n",
    "        # Input size = 64x64x128\n",
    "        x = Conv2D(filters=256, kernel_size=5, padding='same', strides=(2, 2))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(0.2)(x) # size = 32x32\n",
    "        # Output size = 32x32x256\n",
    "\n",
    "        # Input size = 32x32x256\n",
    "        x = Conv2D(filters=512, kernel_size=5, strides=(2, 2), padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(0.2)(x) # size = 16x16\n",
    "        # Output size = 16x16x512\n",
    "\n",
    "        # Input size = 16x16x512\n",
    "        x = Conv2D(filters=1024, kernel_size=5, strides=(2, 2), padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(0.2)(x) # size = 8x8\n",
    "        # Output size = 8x8x1024\n",
    "\n",
    "        # Input size = 8x8x1024\n",
    "        x = Flatten()(x)\n",
    "        out = Dense(1)(x)\n",
    "\n",
    "        net = Model(inputs=inputs, outputs=out)\n",
    "\n",
    "        return net\n",
    "    \n",
    "    def generator():\n",
    "        dropout_prob = .4\n",
    "\n",
    "        # Input size = 100\n",
    "        inputs = Input(shape=(100,))\n",
    "        x = Dense(8*8*1024, input_shape=(100,))(inputs)\n",
    "        x = Reshape(target_shape=(8, 8, 1024))(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(0.2)(x)\n",
    "        x = Dropout(dropout_prob)(x)\n",
    "        # Output size = 8x8x1024\n",
    "\n",
    "        # Input size = 8x8x1024\n",
    "        x = Conv2DTranspose(filters=512, kernel_size=5, strides=(2, 2), padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(0.2)(x)\n",
    "        x = Dropout(dropout_prob)(x)\n",
    "        # Output size = 16x16x512\n",
    "\n",
    "        # Input size = 16x16x512\n",
    "        x = Conv2DTranspose(filters=256, kernel_size=5, strides=(2, 2), padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(0.2)(x)\n",
    "        x = Dropout(dropout_prob)(x)\n",
    "        # Output size = 32x32x256\n",
    "\n",
    "        # Input size = 32x32x256\n",
    "        x = Conv2DTranspose(filters=128, kernel_size=5, strides=(2, 2), padding='same')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = LeakyReLU(0.2)(x)\n",
    "        x = Dropout(dropout_prob)(x)\n",
    "        # Output size = 64x64x128\n",
    "\n",
    "        # Input size = 64x64x128\n",
    "        x = Conv2DTranspose(filters=3, kernel_size=5, strides=(2, 2), padding='same')(x)\n",
    "        out = Activation('tanh')(x)\n",
    "        # Output size = 128x128x3\n",
    "\n",
    "        net = Model(inputs=inputs, outputs=out)\n",
    "\n",
    "        return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, _), (_,_) = tf.keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0722 01:33:53.836092 13092 deprecation.py:323] From <ipython-input-4-52f28e52ec3f>:16: map_and_batch (from tensorflow.python.data.experimental.ops.batching) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.data.Dataset.map(map_func, num_parallel_calls)` followed by `tf.data.Dataset.batch(batch_size, drop_remainder)`. Static tf.data optimizations will take care of using the fused implementation.\n"
     ]
    }
   ],
   "source": [
    "def preprocess_fn(image):\n",
    "    x = tf.reshape(tf.cast(image, tf.float32), (32,32,3))\n",
    "#     x /= 255\n",
    "    x = 2*x/255 - 1 # convert image to [-1, 1] range\n",
    "    x = tf.image.random_flip_left_right(x)\n",
    "    x = tf.image.random_hue(x, 0.08)\n",
    "    x = tf.image.random_saturation(x, 0.6, 1.6)\n",
    "    x = tf.image.random_brightness(x, 0.05)\n",
    "    x = tf.image.random_contrast(x, 0.7, 1.3)\n",
    "    return x\n",
    "\n",
    "real_ds = tf.data.Dataset.from_tensor_slices(x_train)\n",
    "real_ds = real_ds.shuffle(60000)\n",
    "real_ds = real_ds.repeat()\n",
    "real_ds = real_ds.apply(tf.data.experimental.map_and_batch(\n",
    "        preprocess_fn, 64, num_parallel_batches=6, drop_remainder=True))\n",
    "real_ds = real_ds.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wgan = WGAN(real_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 50\n",
      "Step: 51\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output, Image\n",
    "\n",
    "for step in range(0, 15001):\n",
    "    for i in range(0,5):\n",
    "        wgan.C_train_on_batch()\n",
    "        wgan.clip_weights()\n",
    "    wgan.G_train_on_batch()\n",
    "        \n",
    "    if step % 50 == 0:\n",
    "        clear_output(wait=True)\n",
    "    print(f\"Step: {step}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 32, 32, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x19ebe06b438>"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "noise = np.random.uniform(-1.0, 1.0, size=(5,100,))\n",
    "fake_batch = wgan.G(noise)\n",
    "print(fake_batch.shape)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.axis('off')\n",
    "fake_batch = (fake_batch + 1) / 2\n",
    "plt.imshow((fake_batch[3]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
